---
layout: default
title: Managing science for Impact
parent: How do we get there, Handbook for Scientists and Managers
nav_order: 18
---

# Managing Science for Impact

Recently, an investor friend asked for my feedback and help. It was with one of his healthcare companies, one that was mostly based on the commercial application of a research discovery. I can´t share the details, but the goal was a profound advance in the treatment of a major type of deathly disease. Getting an effective treatment would be a hard process, as it involved some behavioral changes, but the upside had a huge potential benefit. A medical Moonshot. The main engineer, a published researcher that was making his name on this project, was the linchpin of the company, but it was impossible to manage. No commitment to timelines, no real justification of budget needs, incomplete documentation, unable to externalize tasks to move faster, and endless research side projects were some of my friend’s concerns. I felt somewhat identified with that scientist. He shared with me some of his notes. It was a very interesting hypothesis, and it involved more exploration before exploiting the intended product. When we went through the ideas of this book it became clear that my friend was falling into the Vannevar Bush “free play of the free intellect[[1]](#_edn1)” trap and he needed a more strict Moonshot attitude. It was also clear that the incentives were misaligned, and the scientist was closer to the HIV story we explored on paving a recognition path than to landing the Moonshot. Despite having invested for years, my friend decided to pull the plug, mostly due to his inability to manage this scientist. This is not an isolated case. In fact, I have found this struggle with businesses trying to manage scientists. It is not hard to create measurable scientific research results, but this is harder to create measurable scientific impact results. This struggle is about leveraging a scientist to _land_ a Moonshot, having them working directly with company executives, elected politicians, due diligence teams or into operational management.

In many cases a scientist will likely be a research scientist, and turning them into an impact scientist is a gradual process over years and even over different jobs. I have collected some recommendations based on my own experience, and many interviews across fields, countries and fields of application:

#### If you are an academic

_Seek borders, cross boundaries._ Test your skills developed in one field/place/topic into another one. It helps to see the value of the skills versus the knowledge. E.g. apply your image analysis skills of the sun on an Earth Observation image of the devastation of the hurricane to support response operations with better images. Use your experience making bacteria cultures to figure out how you could grow a plantation faster. I believe innovation can often be found across the interface or crossovers of knowledge or skills applied from one domain on different domains. Breaking assumptions is also another source of innovation, and crossing over to different domains can help identify and break assumptions.

_What you built beats degrees on your curriculum._ When hiring, organizations need an individual who will solve issues. _Their_ issues. And they usually have little time and several candidates. Hiring is a substantial investment of time and effort, so in case of doubt, it is easy to pass. If potential hires can present a project they did from end to end, they are much keener to listen. Especially at the interview stage, an employer needs to know an employee can be useful on day one, not how many degrees or where you did them. Day one, and day “one hundred.” This hundred-day test refers to the perceived ability of a new employee to be much more effective as it quickly learns on the first weeks and months. Pragmatically, degrees are more a presumption of skills, and ability to learn, than proof of applicable knowledge. More so when some of the most demanded skills are not part of official degrees, like data science.

_Be humble; knowledge is less transferable than skills._ Like the point above, I´ve have found especially difficult to accept that credentials, respect, and prominence in academia is based on very different set of rules as many other places. You might know what very few other academics know in the world, and might have worked in very selective places, but you will need to accept that you need to prove yourself again from scratch. Don´t worry, if you are in the right place, your skills—more than your knowledge—will fast-track the process. It’s not the fancy degree, it is the scientific ability to use those skills to absorb complexity and figure out a solution that will set you back on track.

_Being critical ≠ being productive_. Academics are good at finding and poking holes, at seeing if there is something wrong or weak, and what an ideal solution could be. Sometimes it is much better to focus on the balance of what already works and understand if that is enough for the scope of the purpose. This is especially important when working in teams, professional advice is important, but getting things done is better than doing things better. “Perfect is the enemy of good.” Good enough is good enough, there will be time to make it better, if it makes sense.

_Disagreeing is often the lazy excuse_. There are many more ways to disagree than there are ways to a agree with something or someone. It is easier, an arguably intellectually lazier, to highlight a particular overlooked weakness and disagree. Yet progress generally depends on finding general agreement. Therefore, it is often harder to figure out how much agreement there is in the particular issue, and if that common ground is enough for the purpose of moving forward within the scope of the issue at the moment.

_Learn to listen_. Most people listen reactively. Most people do that most of the time. We listen thinking what to add to the conversation, so we tend to interrupt when we have something we feel is relevant enough. I believe this is a consequence of the critical thinking mentioned above. Learning to listen means listening along, paying attention, and not interrupting unless really needed. Sometimes words are bad conveyors of complex meaning, and it takes time to rightfully express oneself. Listening like this is harder than it seems, especially when the person talking is not as focused on facts as we researchers like to do.

_Avoid paternalism_. I have found that many scientists, myself included, fall into the trap of “I am the expert, so do what I say, because if you knew what I know, you would do what I say.” Usually scientists might know part of the picture, but there are other perspectives to consider. Even cases where the most appropriate thing to do is contrary to what the available data says. For instance, climate change, is not about just stopping the emissions of carbon dioxide by closing all factories right away. The issue is also about incentives, jobs, political agendas, diplomacy, etc. As much as scientist push out data arguing for the need to stop carbon emissions, progress is happening at venues where science is only part of the mix, such as within the United Nations or at the World Economic Forum in Davos.

_Push less, pull more_. This is something I´ve found with many formerly academic colleagues. We like to push from our knowledge, to explore the unknown, to understand. Often, we need the opposite. To pull from the solution to a problem. To start with the goal, like in the case of the Moonshot, and then backtrack what needs to happen until we can link to what we do today. Pulling from the end goal means making sure the output and the outcome are linked, it means there is less chance for pure exploration, but also draws a clear path of accountability. Having an explicit “pull” attitude in your projects, even research projects, helps reduce the risk of stagnation or the focus on the process itself. Same goes for communications, we tend to push a content when we communicate, when it is much more efficient to place yourself on the receiver side and think how to pull the content. This is especially important when speaking for example with policy makers, CEOs, or in general those who need to handle many dimensions at once when they also need to make decisions quickly.

  

#### If you teach scientists

_Ask yourself if your goal is to create scientists or academics._ Is the goal of your unit or department to apply science to create new knowledge, or to make scientists (people with skills in science, regardless of where they apply their expertise)? We tend to conflate that academia is the place where we make both things: academic research, and education of scientists. If you are creating scientists, build a measure of success that is not about number or papers, citations or funds given to the institution. Maybe it is based on pilot projects, or how many students are happily employed one year after graduation.

_Think about mental health and well-being beyond managing crises_. It is good to have a system to check on people and give professional help in crisis, but this also about teaching tools for mental health, resilience, stress management, introspection, and mindfulness on a continuous and ongoing basis. Several really good scientists I have worked with ended up suffering from mental health issues due to stress, anxiety, and other factors that ended their intended careers, and placing them in a future for which they were also unprepared for.

_Measure happiness_. I am a huge proponent of measuring happiness levels. Happiness is holistic, subjective, ill-defined, and partly a personal decision. This usually makes for a very poor indicator. However, lack of happiness usually happens for a concrete set of impediments one can work with. You can’t be happy if you are very stressed, if you are sick, if you have an issue with your advisor, if your commute is two hours, if your grant didn’t come through. Building a sense of happiness means working to remove the barriers that prevent people to seek happiness, especially those affecting the professional goals of your students.

_Include soft skills in the training_: Hire an expert to improve communications skills. A communications professional, by definition, is ideal, but someone like an actor, or a radio or TV anchor could give really good insights. Create more public-facing student-organized events…

_Provide visibility to non-research options_. Contact companies and alumni to advertise their positions, internships or particular projects. Invite alumni who left academia to share their experience. Form alliances and agreements with other non-research institutions to share openings, news and conference opportunities. Break the assumption that the highest goal of science is to be an academic.

_Provide mentors AND buddies_. The figure of a mentor is important, as is someone with more professional experience to help you make decisions or work with concerns. Your PhD advisor is usually your mentor. The figure of a buddy is someone more of a fellow peer that has just been where you are, and so it can provide more personal and candid advice. A buddy could be a recent graduate or a post-doctoral student.

  

#### If you manage a scientist

_Address biases_. Most scientists will come from academic backgrounds trained for research. Understand that their whole professional career they have been incentivized for excellence and abstraction, not timeliness. The incentive is trying to get the right data, in the right format, to address all cases at once, including edge cases, on a clean process. Not to deliver what is needed in a given time window. An academic scientist will get frustrated quickly when their workload is not academic, and has to deal with a forced limited scope, a concrete staged timeline of results, or has to handle soft skills. Communicate and work on these biases early on.

_Leverage on skills_. Academic scientists are, by training, fast learners. Their education was based on tools to absorb complexity and novelty. Therefore, shadowing, and high cadence of rotation on the first few days can work really well. This means having them just sit next to a more experienced colleague and have them ask questions as the work is being done. Rotating quickly on the first few days also helps get a comprehensive picture of the workload and implications, much more than focusing on the in-depth technical details of their particular work-load. In fact, one of their bias will probably be to tend to go down the rabbit hole of increasingly particular and detailed implications of a specific problem, rather than getting just deep enough to solve the problem, or defer the decision to go deeper only if it makes sense to spend the time.

_Round up skills_. An academic scientist is very asymmetrically trained. They might be able to master very specific and valuable skills, but might be unable to present that value to others in an internal event, or be minimally effective at a client or stakeholder meeting. Addressing those asymmetries directly with concrete milestones will speed up the process into a much more effective team member.

_Manage pulling from solutions, not pushing from principles_. Scientists are trained to start from (falsifiable) principles or hypothesis, and then move towards the goal from there. In goal oriented practical settings there are several problems with this approach. For one, principles are most times idealistic conditions than might not include variables than yield the approach ill-defined, biased, or blind to important aspects. When managing from the goal, and pulling from what is needed, we are forced to consider all aspects, and in a way that they bring it closer to the goal, it also prevents secondary findings which might be interesting but not conductive to the goal. It also makes it easier to budget time, resources, and potential problems. Pulling from the goals is saying “What do you need so I can get this done by this date” instead of “What do you think about this issue? How can we fix it?”

  

{% include more.md %}
